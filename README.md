# ReLEP
A repository for ReLEP
This ReadME will serve two purposes.  First it will describe as effectively as possible, how to download and run this code.  Second, it will translate the individual pieces of code into plain english, for those who are unfamiliar with code, and do not have time to pick apart exactly what is happening.


To download and run this code:

This section still needs to be developed.






Translation of the individual scrpts  (the text below is from the beta version and requires MANY updates.  Take the folowing with a grain of salt:

Every step in the program is synonymous with a question we ask ourselves every time we write a LOE, but we do it almost without thinking.  Some of these questions may seem obvious, but computers are not good at obvious things, they are only good at doing what they are told.  They only do what they are told to do, nothing more, and nothing less.  This is good because a computer will do each task the exact same way every time and this is good for accuracy.  The downside of this is that you need to tell the computer exactly what to do.  There is no approximate answer to any question.  It must always be yes or no.  That is the main intent of all of the tables we have been creating and reviewing.  They are each lists of “yes” answers to each of the questions the programs asks each row of data.  For example, when the program asks the data “Does this waterbody have beneficial uses?”  It only knows the answer is “yes” because that waterbody is listed in the beneficial use table exactly as it is spelled in the station table.  If there is any discrepancy between them, the waterbody may as well not be there at all.  In other words, the computer only knows the answer to each question is “yes” because we specifically told the computer “the answer to this question is yes in these specific situations and ONLY in these situations”.  For all situations outside this list, the answer defaults to no.
When reading these documents, think of the data loaded as being in one giant table that is split apart and reshaped as it moves through the program.  As the data moves through the program, it goes through a series of filters and splitting tools that categorize the data.  At each filtering step, the data that fits through the filter moves on to the next step of processing.  The data that cannot fit through the filter is exported separately into a file detailing the error (i.e. missing waterbody information).  The intent of the filters is:
1.	To ensure that the program only assesses data that it knows how to assess, 
2.	That each data type is assessed correctly, and 
3.	That all data loaded into the program can be accounted for.  
As the data passes through each subsequent filter, it is gradually reshaped from raw data into finished LOEs.
An alternative way of describing this, is that the program asks each row of data a series of questions in order to determine if it can assess the data, and then how to assess it.  For example, the program asks, “Does this station in this row of data have a CalWQA waterbody?  If yes, does that waterbody have beneficial uses?  If yes, do those beneficial uses have a numeric objective for the pollutant, matrix, fraction, combination listed in this row of data?  If yes, what water quality objective applies to this row of data?”  If the answer to any of those questions is “no”, then the data is exported and is labeled accordingly.     
Similarly, the program will only assess data differently if it is specifically told to do so.  Any data type not specifically called out to be assessed differently will be assessed in the simplest way possible.  For example, as the data moves through the program, the results of that row of data will not be summed with any other pollutants, unless its pollutant name is on the list of pollutants whose results should be summed.  If the pollutant is on that list of summing pollutants, its results field will only be summed with the results fields of the other pollutants that also fall on that list list of pollutants.  Without the list of summing pollutants, this step cannot happen.  Each time a new pollutant, or group of pollutants need to be summed, they must be added to the appropriate list before the program will be able to assess them correctly. 
Occasionally, only a small part of a larger dataset needs to be modified before it can move forward to the next step.  In these circumstances that data is split out from the larger dataset, is modified accordingly, and is recombined with the larger data set again, so it can continue on through the rest of the program.  For example the program asks “Does the result field of this pollutant need to be summed with the results field of another pollutant?”  If the answer to the question was yes, that row of data would be sent to the summing pollutant portion of the program, where the results fields would be summed, the pollutant name would be changed to the summed pollutant name, and combined back into the dataset.
This is the fundamental framework of the program.  Use lists to split, move, and modify values in a dataset.  This requires a large resource investment upfront for creating the lists, and cataloging procedures, but once created, maintaining the lists will be a much smaller task than manually writing LOEs.  Additionally, making sure the value in a table is correct once is much easier than making sure that same value is correct in many LOEs written by hand.  This results in lower workload and higher accuracy of the LOEs produced, and will allow more time for tool creation, and research of higher quality methods for future assessment cycles in way that have previously not been practical.
This document is broken up into six sections.  The first section outlines how the program will derive each of the fields required by a LOE.  This section will be referenced by each of the following five sections.  Those five sections outline the overarching characteristic that defines the data assessed by the module, as well as the specifics of each assessment type.  The generic pollutants and objectives that require no special handling are the easy part of this process.  It is much more difficult to remember and ensure that the more complex pollutants and processes are all captured correctly, so please try to think of these as you read.  In addition, think of easy mistakes that are made often and pollutants that need specific processing that may have been overlooked.  If you think of something, and it is not specifically discussed in the document please ask me about it, because if it is not specifically discussed, it was probably missed.  We may not be able to address the problem this cycle, but at least we will be aware of the problem and will be able to address it as soon as possible.





















LOE Writing steps
Definitions:
Data Frame – more or less synonymous with spreadsheet or table
Split – create a copy of the data that meets the specified criteria to be operated on separately from the data it was split from
Join – add this data back to the specified table based on a set of unique criteria that is common between the two tables (e.g. waterbody, pollutant, matrix, fraction, sample date combinations).  All values in the criteria must match exactly.  Only keeps the records that match between the two tables.
Merge – Synonymous with Join
Anti-join – the opposite of a join.  Creates a list of rows in which there were no matches between the two tables.

1.	Add objectives back to assessed data (if necessary), remove all excess fields except Objective language, Evaluation Guideline language, Objective Ref, and Eval guide ref.  When these fields are added back the CalWQA reference codes will come with them.  Rename the fields to match the name in LOE uploader tool.  
2.	Lookup and add QA document, and data used information based on project code from data (QARefCode and Data_Used codes in CalWQA) These are tables we will make once the solicitation ends and as we get information processed and entered into CalWQA. 
3.	Create list of stations within a waterbody by joining data with stations table which give all data plus waterbody and waterbody ID, then filter for distinct waterbody, pollutant, matrix, fraction, project code combinations while counting the number of stations that fall under each of these unique combinations.  Create new field called SPATIAL_REP that contains the generic language “The samples were collected at ___ monitoring site(s) (____)” in which the blanks are filled in with Count of stations and list of station codes respectively.
4.	Create a new field called DATA_USED that contains the generic language “Waterboard staff assessed data for _____, to determine beneficial use support and the results are as follows: _____ of ____ samples exceeded the _____ beneficial use for ___.” in which the blanks are filled in with waterbody name, exceedances, samples, Beneficial Use Code, analyte name, respectively.
5.	Calculate the date range (Temporal_Rep) over which data was collected for each waterbody, pollutant, matrix, beneficial use, fraction combination by first calculating the minimum date into one field, then the maximum date into a second field then combine them into a single field called Temporal_Rep that inserts the max and min dates into the generic text “Samples in this LOE were collected from _____ to ______.”
6.	Change field names to match exactly with the spelling of the LOE uploader template
7.	Add Assessment status field -> “LOE in Progress” will be status for all LOEs at output
8.	Add date created field, will be the date on the computer the day the LOEs were written
9.	Add Region number field to LOE - this will be part of the program for each region; will not require users to do anything.
10.	Add Author field -> the user will enter their username before they run the program – their username will be their CalWQA username but could technically be any text the user wants to enter
11.	Add matrix field.  Except for toxicity, the matrix field will be added according to the module the assessment is in.  Tissue will be assigned tissue, sediment to sediment, and so on.  The toxicity module will assign the matrix based on the matrix field in the data itself.
12.	Pollutant sub group will be assigned the same way as matrix, and this field is more or less synonymous with matrix, but is present to allow the assessor to add more detail if necessary.  It will be assigned based on the module.  This could be changed at the end if the LOEs happened to be specific to another one of the possible values allowed by the LOE uploader tool.
13.	Add migration ID field - > migration ID will be whatever row number the LOE happens to fall under. This is a generic code that is only used by the LOE uploader tool to track the uploading process.  It disappears once the LOEs make it into CASSM.
14.	Add environmental conditions field – this will be entered by the user once the LOEs are output since it is an optional field – it will be output as a blank field.
15.	Add assessors comments field – this will be blank – the user can either add a comment in here before the program is run that will be the same for all LOEs, or they can wait until after the LOEs have been output to an excel file and change each row individually.
16.	Add data type field -> We are probably going to whittle down the allowed values for this field to just the ones in all capital letters in the CASSM LOE upload template list of values.  This value will be pre-determined based on the module that is running in ReLEP.  Physical/Chemical monitoring for water sediment and tissue, Toxicity Testing for toxicity, Pathogen Monitoring for pathogens.  This is pretty consistent with how the field is used currently.
17.	Reorder the columns so that their order matches the order of fields listed in the LOE uploader template.  Not because it needs to, just because the order makes sense logically for reviewing.
18.	Add data source field which is either SWAMP if the project code comes from SWAMP and Non-SWAMP if the data comes from anywhere else.






Water Matrix ReLEP Steps
Command Definitions:
Data Frame – more or less synonymous with spreadsheet or table
Split – create a copy of the data that meets the specified criteria to be operated on separately from the data it was split from
Join – add this data back to the specified table based on a set of unique criteria that is common between the two tables (e.g. waterbody, pollutant, matrix, fraction, sample date combinations).  All values in the criteria must match exactly.  Only keeps the records that match between the two tables.
Anti-join – the opposite of a join.  Creates a list of rows in which there were no matches between the two tables.
Steps:
1)	Load data and lookup tables into workspace.
2)	Modify data according to QA codes.  There are hundreds of QA codes in CEDEN, which ones are most important?    Any samples thrown out at this step will be exported as a table named “QA Flagged”.  A list of the QA codes and their meaning can be found on the CEDEN website by looking in their list of controlled vocabulary.  Throwing out samples at this step is an option, but if someone wanted this to be done, this is where it would be best to do it.
3)	Remove data collected for QA purposes only.  The only data types that move on from here are the SampleTypeNames Grab, Integrated, and Composite.  It is unlikely Composite sample type will be in water.  Melissa Morris confirmed that integrated samples should be assessed.  They are several samples taken to get average concentrations over an area or over time.
4)	Cut out all data that is missing both a MDL and a RL value, and export it as a table named “Missing ResQualCode.”  Normally a RL can be calculated from a MDL if it is missing, but when both are missing we cannot determine quantitation limits.
5)	Load the rest of the necessary tables for the assessment
a.	Sites table – contains a list of sites from the data and the waterbody they belong to. Used to connect sites to beneficial uses.  Each site should only occur once in the table but each waterbody can occur many times
b.	Beneficial uses – contains a list of waterbodies, WBIDs, and the beneficial uses that belong to each waterbody.  Each waterboy BU combination should be on its own line.  Each waterbody can occur many times, each beneficial use can occur many times, but each waterbody BU combination should only occur once in the table.  Duplicated occurrences will results in inflated sample counts proportional to the number of duplicates.
c.	Summing Pollutants – list of CEDEN pollutants names whos results needs to be summed to calculate the result of some other CalWQA pollutant.  For example Anthracene C1, C2, C3 needs to be summed to become Anthracene.  Each CEDEn anem and Each CalWQA name can appear in the table multiple times, but each CEDEN Name CalWQA name combination should only occur once in the table.  Duplicates of these combinations will inflate the summed result proportional to the number of times the combination is duplicated.
d.	PAH_TEFs – a talbe PAHs and their corresponding toxicity equivalency factors.  This table is used to multiply the results of PAHs by their toxicity relative to benzo a pyrene before summing the results to calculate Total PAHs and compare to the criteria for Benzo a pyrene.
e.	Data references – list of parrent project names from CEDEN, and the corresponding CalWQA references and QAPP documents/CalWQA codes.
6)	Look up waterbodies and beneficial uses for all of the data.  Create a table of data from stations that do not appear in the sites table.  Export this as table “Missing Stations” to be reviewed to see if data is assessable.
7)	Split out the data that will be used to calculate the objectives of the other pollutants (hardness, pH, DOC, etc.).  This step must be done here because otherwise we risk these rows being thrown out during the objective lookup step if these pollutants do not have objectives.  For example, hardness does not often have an objective, but it is often used to calculate other objectives.  The same may be true of temperature (for MUN).
8)	Split out Total Ammonia data whose samples were not also reported for un-ionized ammonia.  Using the analytes Ammonia N Unionized, Ammonia as NH3, unionized, and Ammonia as NH3 as the definition of unionized ammonia, and Ammonia as N as the definition of Total Ammonia.  Projects that assessed both total ammonia and unionized at the same time will not be included in this step.  For projects that only reported Total Ammonia, split out these rows as well as the corresponding pH and temperature data.  Use the formula listed in the August 1999 EPA guidance document to convert these data to unionized ammonia.  Although EPA has since released a newer ammonia guidance document (EPA 2013), this document only revised the formula for the criterion calculation (step 7) and did not revise the formula for converting to unionized ammonia.  The formula for the fraction of unionized ammonia = 1/(1+10^((.09018+(2729.29/(273.2+Temp)))-pH)).  This is multiplied by the total ammonia concentration to give the un-ionized ammonia concentration.  The name of the pollutant is changed to reflect this, and then the data is added back to the main data frame.
9)	Calculate hardness adjusted objectives. For hardness dependent objectives adjust the result field to convert between fractions.  If that station only has total metals reported, you must convert the total fraction to dissolved in order to assess for aquatic life BUs.   
a.	Distinguish between whether or not the conversion is necessary by creating a list of stations with dissolved fractions and then doing an anti-join with the data to get a list of stations with ONLY total fraction (because stations with only total fraction will not be in the dissolved list) and then converting from there.  If dissolved metals data is reported this conversion is not necessary.
b.	Change all hardness results over 400 to be equal to 400.  Change all results that are non numeric, or otherwise not a number to be 100 (each different analyte needs its own formula to calculate the objective so each pollutant will be split out into its own table and operated on separately).  The metals that will be hardness adjusted are listed in the CTR.  They are Cadmium, copper, chromium, Chromium III, Lead, Nickel, silver and Zinc
c.	Add a field to each row of data for hardness by joining the previously split out hardness data to each of the analytes that have hardness dependent objectives do a keep all join so that we keep rows of data that were submitted without hardness data.  These rows will have NA for the hardness field.  Convert these entries to have a value of 100.  Each row of hardness data will have a new field added called “objective” that contains the result of the calculated objective.  We then add the objective, objective reference number beneficial use that these objectives apply to (Cold and WARM).  To essentially create a mini water analytes table that contains objectives that change depending on the data in the data set, and will be joined back into the main data frame using analyte matrix fraction station code and sample date combinations.  This table will be called “hardness dependent objectives”.  Hardness dependent objectives only apply to aquatic life Bus for metals (as far as we know so far) but they apply to all metals in the CTR and NTR so that makes things easier.  The hardness dependent objectives are only applicable to the aquatic life beneficial uses (COLD/WARM).  
10)	
11)	Adjust units so that the objective and result field match – there are a LOT of funky units in the CEDEN constituent list but I am only going to have the straight forward units converted for each of the matrices because there are literally 400 matrix unit combinations within CEDEN.  For water matrix all results will be converted to ug/L.  The initial units I will have ReLEP convert in the water matrix module will be pg/L, ng/L, mg/L, and g/L.  These are the most likely units to be seen.  They will be converted to ug/L which will be the unit all objectives have within the ReLEP analytes tables for water.  Temperature data that comes in as degrees Fahrenheit will be converted to degrees Celsius.  The other funky units will be taken into account after the solicitation as we receive data for that unit.  For example, it is very likely that data with the matrix “Water” and the units of mg/kg dw is a mistake or at least, is not useable data so there is no reason to preemptively handle this data.  Many of the other funky units seem to belong to analytes that do not have objectives either (such as density, salinity, etc.)
12)	Lookup waterbody, Beneficial Use, Objective.  Each of these is a different step, and each step looks up matches to add data.  For example, to look up waterbodies we compare station codes in the data with station codes in the waterbodies table.  Each match will have the appropriate waterbody and waterbody ID attached to the end of the row of data.  Any row of data that has a station code for which there is no match, partial or otherwise, within each of these “lookup” steps, will not continue on to the next step of the assessment process (e.g. beneficial use lookup based on waterbody information), instead it will be output into a separate folder for “unassessed data” so that it can be reviewed by the assessor to make sure that the row of data not being assessed was intentional.  This is less likely with waterbody and beneficial use lookup but is much more likely with objective lookup.  It is likely, if not guaranteed that there will be analytes in the CEDEN data for which we do not have objective.  This will be helpful for identifying and prioritizing future objective/guideline research, as well as identifying any missing objectives that should not be missing.  Before completing the objective lookup step, use a list of pollutant names that must be summed to split out this data into their own data frame where the results field will be summed based on their pollutant group (e.g. Anthracene C1 will be summed with other pollutants in the Anthracene group).  This table will run parallel with the main data table for the next few steps.
13)	Split out total ammonia data between the beneficial use lookup and the objective lookup.  Only split out the ammonia data for the Cold or Warm beneficial use.  Join the results field of pH and Temperature that correspond with each of the ammonia samples to calculate the objective based on the CCC formula described in the 2013 Total Ammonia Criteria released by EPA.  The formula is (.8876)*((.0278/(1+10^(7.688-pH)))+(1.1994/(1+10^(pH-7.688))))*2.126*10^(.028*(20-T)).  Calculate the objective then remove the results field for the pH and the Temperature, then join the data back with the main table.   This is different from the conversion of total ammonia to unionized ammonia, because the first is a conversion of a concentration from one form of ammonia to another while this step calculates the water quality objective for total ammonia.  Some regions only have one objective and not the other.  We may not need both, but some regions may want one over the other, or they may want both.  Unionized ammonia is really the toxic part for fish, not total ammonia, but this formula is newer, I can see both sides.  This way they can choose.
14)	Create a table that counts the number of stations used in each waterbody, pollutant, matrix, fraction, project code, combination.  Record the code for each of the stations in a list to be attached later (e.g. StationCode1, StationCode2, StationCode3, etc.).  
15)	Correct for quantitation limits –using the following rules: If MDL and RL are missing, remove and place the data into a file named “Missing MDL” to be inspected later by hand to determine if it is useable.  If RL is blank, calculate it by multiplying MDL by 3.18.  If RL is less than the objective and ResQualCode is ND or DNQ the result becomes ½ the MDL and thus the sample counts as clean. If RL is greater than the objective and ResQualCode is ND or DNQ sample is thrown out because there is no way of knowing if it exceed or not.   Keep track of the number of samples that end up being thrown out as a result of this, and put the count in the “Data_Used” Section – count them and include some text that says “___ samples were collected, but ___ samples had to be thrown out due to quantitation limits” – open to suggestions on the exact wording here, but it is easy to change.  Put these samples in a table called quantitation discards or something along those lines.  They will be counted in a similar away to the station information that will be joined back in based on waterbody Pollutant, Matrix, Fraction, Beneficial Use, combinations.  The general procedure to create a new table of all the samples (with Waterbody, Pollutant, Matrix, Fraction, Objective, Project Code, Fields as well) in which the RL is greater than the objective and the ResQualCode is either ND or DNQ.  Next step with this table will be to collapse these entries based on unique Waterbody, Pollutant, Matrix, Fraction, Project Code, and count up the number of samples.  We then join this to the main table once the rest of the assessment has been completed but before the final LOE potion is created.   We will do an antijoin at the end of the assessment to make sure we catch situations when all samples that would have otherwise been included in a LOE are thrown out by the quantitation check.  It is possible if not likely that thrown out samples may be the ONLY samples for the data set.  This is especially true with pesticide data.  These LOEs will be given sample and exceedance counts of zero of zero before being added to the main LOE table.  This step has to be at this point in the program (not earlier) because we need to know what the beneficial use, and objective for the data is before we can check the quantitation limits.  Although it is possible that this could be done a little earlier, it must be done after step 2 and later in the process because, we need to modify some things (change the units, calculate unionized ammonia concentrations and change the name).  If we check quantitation limits before these steps have happened, the data may not match the objective, and may be thrown out (or kept in) inappropriately.  The way I have it set up now is, if RL is missing but MDL is present, we automatically calculate the RL based on the MDL by multiplying it by 3.18, but if they are both missing the data will need to be inspected by hand.  In the case where both MDL and RL are missing we would look in the QAPP to see if we could determine the RL or MDL and then enter it into the data by hand before running it through ReLEP again.
16)	Remove replicates (samples from same day) by averaging the results field based on all of the fields that are needed from this point to the end of the process – Waterbody, Pollutant, Matrix, Fraction, Station Code, Sample Date,  MDL, RL, ResQalCode, etc.  Split out total ammonia again; recalculate objectives the same way as in step 5, then join back into main table.  This allows us to calculate the objectives for each individual pH total ammonia combination (which should give us the most accurate concentration since ammonia is pH dependent, and then to calculate the ammonia concentration averaged over the day, which gives us the replicateless ammonia concentration for that station, day combination.  This might be overkill but it is just as easy, and I do not think it will do harm, and I think it is important to consider each ammonia, pH; Temperature unit as one each time it appears.  The objective was calculated the first time so that we could confirm that the result fell within quantitation limits.  Now, the rows that made it through quantitation are being averaged if more than one sample was collected on the same day, since the results have been averaged for each of the samples (pH, Temperature, ammonia), it seems to make more sense to me to just recalculate the objective rather than averaging the previously calculated objective, especially since it is such a complicated formula, it is not clear how changes in one variable may affect the result.
17)	Add the results field together for the different pollutants that need summing (PCBs, Aroclors, Chromium, DDTs Chlordanes, Endosulfan I and II, etc.).  This data frame was created after looking up Waterbody and Beneficial Uses, but before looking up the Objectives.  Will split these rows out into their own data frame from the quantitation check, and it will run the same way, except that this time if RL is less than the objective, and ResQualCode is ND or DNQ, then the result will get changed to a zero as per management guidance for quantitation limits.  The objectives will be looked up based on the pollutant group name (e.g. Anthracene C1 objective is looked up using the name Anthracene).  Run the quantitation check on the data.  Remove replicates based on the CEDEN pollutant name (not the pollutant category, e.g. Anthracene).  Then the results fields will be added together based on the pollutant category (e.g. Anthracene).  PCBs and Aroclors will be summed separately and differently depending on the Beneficial Use that applies to them.  Data for COLD and WARM Bus will only sum Aroclors 1242,1254,1221,1232,1248,1260, and 1016, and will not sum the individual congeners.  COM and MUN BUs will sum all forms of PCBs but will sum the Congeners separately form the Aroclors and then the result (PCB or Aroclor) with the highest value for each day will be used in the final assessment.  Join the PCB sums back to the rest of the summed pollutants.  Join all summed pollutants back with main table.
18)	Based on the averaging period specified in the objective (or not), average samples which will result in the correct number and value for the samples taken over each averaging period.  Data with CTR and NTR chronic criteria will be split out and use a 4 day averaging period (total ammonia also).  Is everything else 7 day default averaging period?  What else am I forgetting?
19)	Calculate objectives for pollutants that are dependent on the concentration of other analytes (hardness, pH, others?)
a.	To do this I will first split out the data for each of these types into their own table (hardness needed data, pH needed data, oc normalized data, toxicity equivalency data)  Each of these will be operated on separately
b.	Cut out pentochlorophenol and split out the corresponding pH entries.  Use the formula from the CTR for aquatic life beneficial uses CCC=exp(1.005(pH)-5.134).  For rows that do not have corresponding pH value use default value of 7.8.  Ignore fraction since it is not specified in the CTR but write a separate LOE for each fraction if there happens to be both in the data set.
c.	Total Ammonia – a 30 day rolling average will be calculated which means that there will be up to 30 samples within a 30 day period, and that each within that period may be used in the calculation of up to 30 averages.  These averages will be compared to the objectives calculated post replicate removal.
d.	Are there other specialty assessment types within water that I am forgetting?
20)	Split out DO from the main table count exceedances the same as would be done for other pollutant except that the < will be facing the other direction from most other pollutant since the objective states DO shall not be depressed below the objective
21)	Split out pH rows from main table – pH objective will be split into two rows for each beneficial use since pH objective are generally “not less than ___ nor greater than ___” one row of the objective will be the less than and one row will be the greater than.  So each row of pH data will be duplicated when merged with the objective table.  The exceedance will be counted twice, once for the greater than, and one for the less than objective.  These two rows will then be aggregated together by taking the max of the exceedance row which will be either a zero, or a 1.  So if an exceedance is there it will be counted as a one, and if it isn’t it will be counted as a zero.
22)	Split out aquatic life (Cold/Warm) beneficial use data for chromium data which does not specify if the data is for Chromium VI or III because it must be assessed differently.  According to management direction add Chromium III as a secondary objective to these rows of data.  If the row does not exceed for either chromium VI or chromium III sample is considered clean.  If the result exceeds EITHER the Chromium VI or Chromium III criteria the sample must be thrown out because we do not know which species is causing the problem.  Do we keep track of the samples that are thrown out here?  Do they need to be placed in the LOE?
23)	Once all specialized pollutant types have been operated on so that they have the appropriately modified results and objective filed combine them back into the main data frame (making sure that the unmodified rows of the specialized pollutants have been removed first) so that samples and exceedances can be counted.
24)	Compare to the objectives to result
25)	Add pH, DO, and Chromium rows back into the main table
26)	Count exceedances
27)	Count samples
28)	Count samples thrown out
29)	Add back cases where all samples were thrown out due to discarded sample being the only samples present for a given Waterbody, Pollutant, Matrix, Beneficial Use, combination (quantitation limits).
30)	Write LOEs – See write LOE document


















Sediment Steps
Definitions:
Data Frame – more or less synonymous with spreadsheet or table
Split – create a copy of the data that meets the specified criteria to be operated on separately from the data it was split from
Join – add this data back to the specified table based on a set of unique criteria that is common between the two tables (e.g. waterbody, pollutant, matrix, fraction, sample date combinations).  All values in the criteria must match exactly.  Only keeps the records that match between the two tables.
Anti-join – the opposite of a join.  Creates a list of rows in which there were no matches between the two tables.

Steps:
1)	Load Data
2)	Remove samples with a “J” QA code, or other QA code from the CEDEN list that should either not be used, or needs further analysis before it can be used.
3)	Remove samples that do not have a result.
4)	Filter data for data with the MatrixName “Sediment” only.  The sediment, <63 um matrix should not be used because it is not representative of the sample.
5)	Remove all samples with sample type other than Grab or Integrated
6)	Export data that is missing ResQualCode, RL AND MDL export it as a table names MissingResQualCode.  Normally a RL can be calculated from a MDL by multiplying MDL by 3.18 if RL is missing, but when both are missing we cannot determine quantitation limits.
7)	Multiply MDL by 3.18 to calculate RL IF RL is missing from that row of data, as per management guidance document on quantitation limits.  3.18 comes from the degrees of freedom of the standard number of replicates used when calibrating lab equipment.
8)	Load the rest of the tables used in the program. 
a.	Sites table – list of station codes, and the CalWQA waterbody and WBID the station belongs to.  Each station should only appear once in this table, but each waterbody/WBID can appear many times
b.	Beneficial Use Table – contains a list of CalWQA waterbodies and the BUCODES associated with them.  Each Waterbody, WBID, BUCODE combination has its own row, and should only appear once in the table.  Each waterbody and WBID can appear many times in the table, as well as each BUCODE.  This table is used to look up the beneficial uses associated with each station, by first using the sites table to lookup the waterbody, and then using the BU table to lookup the beneficial use table.  Each row of data will be duplicated for each beneficial use that is applied to it.  Right now the program does not recognize the NA beneficial use because it thinks it is a NA.  The easiest way to fix this would be to change the NA beneficial use in all of the ReLEP table to be “NA”.  I have not done this because I have yet to see an objective use this BU, and making the change would be tedious, and would require post processing of the LOEs prior to loading them to CASSM to remove the “” from either side of the NA.
c.	Analytes – contains a list of pollutants, beneficial uses, and the objectives associated with them.  Each pollutant beneficial use combination should only appear once in the table.  The exception to this being pH which appears twice but each appearance has a different alias.  This is because pH has an acceptable range whereas most other pollutants either have a floor or ceiling.
d.	Summing Pollutants – this is a list of pollutant names as they appear in CEDEN and the name of the pollutant as it appears in CalWQA/Analyte table that the CEDEN pollutant should be summed and combined with.  Each CEDEN name and CalWQA name combination should only appear once in the table, but each CEDEN name can appear multiple times, as well as each CalWQA name, just not the same combination.
e.	Self_names pollutants – this is a list of pollutants whos summing name, calwqa name, objective name are all the same.  This list is used to avoid double counting
f.	Data references – list of ParrentProjectNames, CalWQA reference codes, and associated QAPP codes/information.  We split the data apart by using unique parent project names and loading the split data in to calwqa this way.  Each parent project name should have a CalWQA ref code, and a QAPP ref code.
g.	OC Normalizing – list of pollutants that need to be OC normalized before assessment
9)	Lookup waterbody and Beneficial Use – export rows of data that do not have station information (called Missing_Stations)
10)	Split out the pollutants that are used to calculate objectives (Total Organic Carbon) into their own data frames
11)	Adjust units of results RL and MDL of the main data frame so that the objective and result fields match.  For now I will only be converting the units; g, mg, ug, and pg to ug/Kg.  There are many other units in CEDEN but many are units that do not mean anything to the sediment assessment process.  All data with units not in the above list will be cut out of the main data frame and exported as a table named “Funky Units” that will be reviewed later.
12)	Export total organic carbon data that is not report with the units % dw.  (This is very unlikely to have anything in it, ever.)
13)	Split out the data that needs to be OC normalized and add the TOC result to the end of this table.  Adjust the Result, RL and MDL of the pollutant with the formula OC_Normalized_Result=(Result/(TOC %/100)).  Basically deivide the result by the TOC % as a deciman.  So 50% TOC would become .5
14)	Add OC normalized data back to main data frame
15)	Split out rows of data for pollutants that need summing.  Convert the pollutant name to the summed version of the pollutant (e.g. anthracene C1 will become anthracene).  Lookup the objective based on the summed pollutant name.  Run the following steps in parallel to the main data frame.
16)	Split out PCB and Aroclor data for the cold and warm beneficial uses.  Lookup objective, run quantitation check then sum each of them separately for each day.  Finally, take the maximum result (Aroclor or Congener) for each day and apply it as the PCB result for that day.
17)	Add the objective according to the beneficial use it applies to the rest of the summing pollutants correct the results based on quantitation limtis. Change all results less than zero to be equal to zero.  This will only happen to samples that are ND, DNQ, or otherwise not a level recorded by equipment.  These results are essentially equal to zero as far as quantitation is concerned, and this allows us to remove replicated before quantitation, and will allow us to sum appropriately.  Do not sum the results of rows that cannot be quantified.  Remove replicates (Same station analyte, day beneficial use, etc).  Create a table of data for summing pollutants that do not have an objective to be exported later as SummingPollutants_No_Objective.
18)	Correct for quantitation limits but convert all results to zero for both imperfect scenarios as well as for the perfect scenarios because both results are estimated, and should not be counted towards an exceedance.
19)	Sum the results based on the name as it will appear in CalWQA. Anthracene C1 will be summed with other Anthracenes and will end up with the pollutant name Anthracene.
20)	Remove “Self-Named” pollutants from main data frame.  These pollutants whos name appears identically in the raw data, the summing pollutant list, CalWQA, and Objective table.  Failing to do this would result in samples being counted twice.  An example of this is Anthracene.  The anthracene objective is equal to the sum of Anthracene, phenanthracene/anthracene C1, C2, and C3.  The objective in the table is listed as Anthracene.  Since we must sum all of those pollutants together, and then compare to the objective
21)	Add PCBs back to the summing pollutant table
22)	Create a table that counts the number (and codes) of stations used in each LOE –the “LOE” will be defined as each Waterbody, pollutant, matrix, fraction, project combination.  Create a list of all of the stations used in each LOE to be attached later.  Do this for the SummingPollutants table as well as the main data table, combine the two stations list together afterwards.
23)	Correct for quantitation limits of the main data table–using the following rules: For the main data frame, If RL is less than the objective and ResQualCode is ND or DNQ the result becomes ½ the MDL and thus the sample counts as clean.  For summed pollutants, if RL is less than the objective and ResQualCode is ND or DNQ, the sample result gets changed to zero.  For both main data frame and summed pollutants, if RL is greater than the objective and ResQualCode is ND or DNQ sample is thrown out because there is no way of knowing if it exceed or not.   
24)	For the main data frame only, keep track of the number of samples that end up being thrown out as a result of this, and put the count in the “Data_Used” Section – count them and include some text that says “___ samples were collected, but ___ samples had to be thrown out due to quantitation limits” – open to suggestions on the exact wording here, but it is easy to change.  Note: When coding the text into the Data_Used, or other fields, you must use the command paste0.  When you use this command, you must put all text to be inserted into this field on the same line of code in the coding document.  If you do not, the exported text will have line breaks in it and this will ruin the usability of the export. Put these samples in a table called quantitation discards or something along those lines.  They will be counted in a similar way to the station information that will be joined back in based on waterbody pollutant matrix fraction beneficial use combinations.  The general procedure to create a new table of all the samples (with waterbody, pollutant, matrix, fraction, objective, project code fields as well) in which the RL is greater than the objective and the ResQualCode is either ND or DNQ.  Next step with this table will be to collapse these entries based on unique WB, Pol, Matrix, Fraction, project, and count up the number of samples within each of these groups.  We then join this to the main table once the rest of the assessment has been completed but before the final LOE potion is created (with calwqa ref numbers and the like).   Make sure to also do an antijoin (table of rows that do not have matches) at the end when this is joined back with the data, to capture the rows of data that should have had an LOE written for it, but did not because all samples received were thrown out due to quantitation limits.  It is possible if not likely that thrown out samples may be the ONLY samples for the data set.  This is especially true with pesticide data.  These LOEs will be given sample and exceedance counts of zero of zero before being added to main table.  Also calculate the max and min dates for the samples that were thrown out do this using the same criteria to calculate exceedances for an LOE (Analyte, Matrix, Fraction, Waterbody, WBID, Parent Project Name, Objective, Evaluation Guideline, and codes).  This table will be joined to the quantitation discards at the end once we have determined they were the only samples collected for that LOE
25)	Main Data frame only - Remove replicates (samples from same day) by averaging the results field based on all of the fields that are needed from this point to the end of the process. (WB, Pollutant, Matrix, Fraction, station Code, QA code, Sample Date, MDL, RL, Res Qual Code, etc.)
26)	Add summing pollutant rows back to the main data frame.
27)	Based on the averaging period specified in the objective (or not) average samples which will result in the correct number and value for the samples taken over each averaging period.  All sediment guidelines thus far have no defined averaging period so they will all have the default seven day averaging period applied to them.  This is done by creating an interval that starts on the day the sample was collected and then ends six days later.  The average of the results fields for each waterbody, pollutant, matrix, fraction, unit, combination that falls within that averaging period will be calculated.
28)	Compare to the objectives to result
29)	Count exceedances
30)	Count samples
31)	Add the station information from table created earlier to the generic text “Data was collected from ____ stations (station codes ___...___...___) where the blanks are the number of stations (first blank) and the station codes used in the LOE (the next three blanks)
32)	Calculate the date range applicable to the LOE and insert the max and min values into the generic text “Data for this waterbody was collected from ___ to ___” where the blanks are the min and max dates respectively
33)	Use the quantitation discards and the main table to create a table of LOEs consisting of only samples thrown out due to quantitation issues.  These LOEs will have 0 of 0 samples and exceedances and will have text specifying how many samples were actually taken, but that none could be included in the assessment because of section 6.1.5.5 of the Listing Policy.  This table will need to have DataUsed, Temporal Rep and Spatial rep fields created separately from the main table
34)	Create table of LOEs with samples that also had samples thrown out due to quantitation.  This table will need its data used field to be created separately from the main table and quantitation table because there is extra text for these LOEs.
35)	Change field names so that they match the fields for the CASSM uploader tool
36)	Use lookup table to convert the pollutant names from CEDEN names to CalWQA names
37)	Write LOEs – See LOE writing document




















Tissue Assessment Steps

Definitions:
Data Frame – more or less synonymous with spreadsheet or table
Split – create a copy of the data that meets the specified criteria to be operated on separately from the data it was split from
Join – add this data back to the specified table based on a set of unique criteria that is common between the two tables (e.g. waterbody, pollutant, matrix, fraction, sample date combinations).  All values in the criteria must match exactly.  Only keeps the records that match between the two tables.
Merge – Synonymous with Join
Anti-join – the opposite of a join.  Creates a list of rows in which there were no matches between the two tables.

Steps:
1)	Load the data
2)	Cut out all rows of data that are missing both RL and MDL and export them as a workbook titled “Missing ResQualCode”.  For all rows that have a numeric MDL value, make sure that RL is present for all rows by multiplying MDL by 3.18 if RL is missing
3)	Adjust samples according to their QA codes if necessary.  I think this will mostly involve throwing out samples out that did not meet QA requirements.  Throw out samples with Rejected ComplianceCode.  There are hundreds of QA codes in CEDEN, which ones are most important?    Any samples thrown out at this step will be exported as a table named “QA Flagged”.  A list of the QA codes and their meaning can be found on the CEDEN website by looking in their list of controlled vocabulary.  Throwing out samples at this step is an option, but if someone wanted this to be done, this is where it would be best to do it.  
4)	Check to make sure all data is given in wet weight – if not convert the dry weight results to wet weight.  Procedure: Split out all data from main data frame if it contains the letters DW in the unit field.  This will be the results given in dry weight.   Split out the moisture data from the raw data frame, merge it with the appropriate DW samples by composite id, station code, sample date, and other unique identifying fields, and then multiply the result by 1-(%moisture/100) to give the results in weight per wet weight.  Change the units of those rows of data to reflect this change (i.e. ug/Kg ww).  Add these rows back to main data frame.
5)	Adjust units so that they match the objectives adjust units in MDL and RL at same time.  All Tissue units will be converted to ug/g.  Initially I will be converting; g, mg, ug, and pg to ug/g.  There are many other unit combinations but most seem irrelevant to our assessments.  We will reassess this as we receive data for units that are not within this list.  All rows of data with units not listed above will be cut out of the main data frame will be exported as a table called “funky units”.
6)	Adjust the samples for their fraction (Tissue Name) as necessary.  Ignore prep method (with scales without scales, etc.).  Most TissueNames will be unchanged, but each will be considered independent of the other.  In other words if there are two samples that otherwise match (same station, same composite ID, same pollutant, etc.) but have two different Tissue Names, they will be considered two different samples, but will end up in the same LOE (Matrix name, Tissue, Fraction name either Fish Fillet, or Bivalve depending).  The TissueName that gets assessed by ReLEP will be contained in a list.  TissueNames not on the list will not make it past this step.  Data that does not make it past this step will be exported as a table named “FunkyTissueNames”  A complete list of possible TissueNames can be found in the controlled vocabulary list on the CEDEN website.  So far the TissueNames that will be assessed are fillet, Soft Tissue with Gonads, Whole Organism, Whole Body after removal of head and tail, whole body after removal of head tail and guts, and whole body minus tail.  Values can easily be added or removed from this list upon request.
7)	Split out analytes with TEF’s multiply Result field, MDL, and RL Fields by their appropriate TEF, change their name to the generic all-inclusive name (PAHs, PCBs, etc.), sum the results, and then add back to main data frame.  This will result in the creation of “new” rows of data that consist of TEF modified results.  These rows will be in addition to the original unmodified rows.  The unmodified rows remain in this step because they may or may not have objectives that are independent of the TEF number (i.e. specific to the pollutant assessed).  The rows that are un-impacted by this step, (that do not have an objective specific to the original pollutant name) will be thrown out when the objective is looked up and will be exported as part of the “missing objectives” table.
8)	Use lookup table of species to determine and add a field that specifies if the data requires a fish fillet, or bivalve tissue guideline.  This will be done with a lookup table of all species listed in CEDEN.  Each species in CEDEN tissue data will either be Fish Fillet, Bivalve, Crustacean, or Other.  Any species not on this list will be exported as a table named “New Species in CEDEN” to be reviewed later.  The species listed in the EPA Tissue assessment guidance document will be marked as bivalve species and the bivalve guidelines will be used to assess these species which include species listed in table 3-15 and 3-16 of the EPA Guidance for Assessing Chemical Contaminant Data for Use in Fish Advisories Volume 1, Third Edition (November 2000).  The rest of the crustacean species will be marked as crustacean for the time being.  If at a later date we determine that the guidelines are appropriate for the other species we can update this list to include them.
9)	Sum methyl and total mercury results together.  Change the pollutant name to just mercury?  Join the remaining samples to the main table
10)	Split out the pollutants that need summing, add the summed version of the pollutant name (e.g. Anthracene C1 becomes just Anthracene).  Look-up objective as in step 11 using the summed pollutant name, run quantitation check as in step 12, except throw out samples when RL is less than objective and ResQualCode is ND or DNQ instead of changing the result to ½ of MDL.  Do not keep track of thrown out samples since they are technically subsamples.  Average replicates (samples taken on the same day) based on the original name (e.g. Anthracene C1) then sum the results based on the name of the summed version of the pollutant (e.g. Anthracene).  PCBs and Aroclors will be summed separately (sum of PCBs, sum of Aroclors), and then the maximum value of the two, for each day data is collected, will be used in the final assessment.  Change the name of the analyte name field to match the name of the summed pollutant (e.g. Anthracene C1 to Anthracene).  Add this data back into the main table after step 13.  Note: For replicate removal, and other averaging periods conducted in this assessment composite IDs will be ignored, and samples that are temporally dependent (fall within the same averaging period) that belong to the same species will be considered as being a single sample.  However different species will be considered separate samples regardless of spatial or temporal independence.  This is also true of the same species with different Tissue Names. 
11)	Lookup waterbody and beneficial use.  Using these two fields, the analyte name, and the tissue type (bivalve vs fish fillet) determine the objective
12)	Correct for quantitation limits for all pollutants that do not require summing –using the following rules:  If RL is less than the objective and ResQual Code is ND or DNQ result becomes ½ MDL, and sample counts as clean.  If RL is greater than the objective and ResQualCode is ND or DNQ sample is thrown out because there is no way of knowing if it exceed or not.   Keep track of the number of samples that end up being thrown out as a result of this, and put the count in the “Data_Used” Section – count them and include some text that says “___ samples were collected, but ___ samples had to be thrown out due to quantitation limits” – open to suggestions on the exact wording here, but it is easy to change.  Put these samples in a table called quantitation discards or something along those lines.  They will be counted in a similar way to the station information that will be joined back in based on waterbody pollutant matrix fraction beneficial use combinations.   Also do an antijoin at the end when this is joined back with the data to get a list of samples that should have had LOEs written for them but didn’t because all samples in the data set were thrown out as a result of the quantitation check.  It is possible if not likely that thrown out samples may be the ONLY samples for the data set.  This is especially true with pesticide data.
13)	Remove replicates by averaging samples collected on the same day based on all of the fields needed from now until the end of the assessment.  Add summing pollutants back into this table after this step is completed
14)	Average samples based on seven day averaging period specified by the listing policy
15)	Create a table that contains the number of stations used in each waterbody, pollutant, tissue types (fillet, bivalve),  project code combination
16)	Create a table of fish species for each waterbody, pollutant, matrix, fraction (fish fillet vs bivalve), combination that includes the number of species, and their names involved in the assessment that applies to that “LOE”.  Something like field 1 = number of species, field 2 = list of species names.  This will be added to the data used to assess water quality later on during the LOE writing process.
17)	Create a table that lists species, waterbody, pollutant, matrix, fraction, project that exceeded their guidelines.  This will be inserted into the data used field later with some generic text that is something like “of the data assessed, ___ species (___, ____, and____) exceeded the evaluation guideline for ___.  Where the first blank will be the number of species that exceeded within the LOE, the next three blanks correspond to the species that exceeded, and the last blank corresponds to the analyte that was assessed.  Again, the exact text used here is changeable but will be consistent across all LOEs run through ReLEP that have exceedances.
18)	Compare results to objective
19)	Count exceedances
20)	Counts samples
21)	Write LOEs (see LOE writing document)


















Bacteria Geomean Steps
Definitions:
Data Frame – more or less synonymous with spreadsheet or table
Split – create a copy of the data that meets the specified criteria to be operated on separately from the data it was split from
Join – add this data back to the specified table based on a set of unique criteria that is common between the two tables (e.g. waterbody, pollutant, matrix, fraction, sample date combinations).  All values in the criteria must match exactly.  Only keeps the records that match between the two tables.
Anti-join – the opposite of a join.  Creates a list of rows in which there were no matches between the two tables.

Steps:
1)	Load Data
2)	Modify data cording to the QA codes present.  What QA codes are important with bacteria?  How should I handle the data that has these codes?
3)	Remove replicates (samples from same day) by averaging the results field based on all of the fields that are needed from this point to the end of the process (Station Code, Sample Date, Analyte, Matrix, Fraction, Units, etc.).
4)	Lookup waterbody, Beneficial Use, Waterbody Type (fresh vs marine), and Objective.  I will be using the CalWQA waterbody type, which is the third letter in each Waterbody ID to determine the Waterbody Type that applies to each waterbody.  All types will either be fresh OR marine.  There are two types that will apply to both and those are Estuary, and Tidal Wetland.  This is because their salinity is variable and thus can be either fresh or marine depending on the tide.  We may or may not have salinity data available so it makes more sense to just assess for both.  I think that we should keep both assessments to be loaded into CalWQA, or at least keep the LOE that has the higher exceedance rate.  Data with stations not in the waterbody lookup will be exported as a table called “Missing Stations”.
5)	Add the Objective according to the Beneficial Use it applies to – this will result in fresh waterbodies only having Enterococci, and all marine waterbodies having both E. Coli and Enterococcus data.  Data for other bacteria types will be exported at this step in a table called “No Objectives” that can be reviewed after the assessment is complete. 
6)	A copy of each row will be made for geomean and STV – for example the Enterococci collected January one will now have a row for Enterococci on January 1 for STV and a row for Enterococci on January 1 for geomean.
7)	Create a table that counts the number of stations used in each waterbody, pollutant, matrix, fraction, project code combination.  Record the code for each of the stations as a list to be attached to the LOE later.
8)	Split the data into geomean data frame and STV data frame to be operated on separately.
9)	Convert the date of the geomean data frame into weeks of the year, and then convert the week number into day number by multiplying by 7 and then subtracting six.  This results in all samples collected in the same week having the same “sample date” which is equal to the first day of the week the sample was collected.  For example, all samples collected from January 1 – January 7 will now have the “sample date” January 1.  All samples collected from January 8- January 14 will now have the “sample date” January 8, and so on.
10)	Create new field called geomean interval.   The interval starts with the “sample date” and ends 41 days later.
11)	For each station code, matrix, fraction, analyte, BU combination calculate the geomean for all samples who’s “sample date” falls within the interval created in the previous step. Calculate six week rolling geomeans calculated weekly.
12)	Compare to the objectives to result.
13)	Count exceedances based on waterbody, analyte, matrix, fraction, BU combinations.
14)	Count samples based on same criteria.
15)	Join the two calculations together based on WBID, analyte matrix, fraction, project code, BU combinations.
16)	Write LOEs (See LOE writing document)– insert text into data used field stating that “___ of ___  geomean samples exceeded the objective”
17)	Calculate STV.
18)	For STV data convert sample date into calendar month and year.
19)	Add objectives to data frame based on Analyte, BU combinations.
20)	Compare each row to the appropriate objective, count exceedances.
21)	Add up all exceedances for the month
22)	Add samples and exceedances to the same data frame create new field that is the ratio of exceedances to samples
23)	Count the number of times the exceedance/sample ratio exceeds .1
24)	Count the total number of samples (number of months for which there is data)
25)	Join the samples and exceedance tables back together
26)	Write LOEs (See LOE writing document) – insert text into the data used field that specified that “___ of ___ samples exceeded the STV objective for ___ waterbody”
27)	Combine the geomean and STV LOEs into the same table but distinguished by their assessment type using a temporary field that will be ignored by the LOE uploader tool when uploaded to CASSM – export the table LOEs are done



Toxicity Steps
Definitions:
Data Frame – more or less synonymous with spreadsheet or table
Split – create a copy of the data that meets the specified criteria to be operated on separately from the data it was split from
Join – add this data back to the specified table based on a set of unique criteria that is common between the two tables (e.g. waterbody, pollutant, matrix, fraction, sample date combinations).  All values in the criteria must match exactly.  Only keeps the records that match between the two tables.
Merge – Synonymous with Join
Anti-join – the opposite of a join.  Creates a list of rows in which there were no matches between the two tables.

1)	Load Data, Beneficial Uses table, Sites table
2)	Split out all samples that do not have a SigEffectCode – export this data as a table names “Missing QA”
3)	Remove all samples that questionable “ComplianceCode”.  Export this data as a table named “Review Test Procedures”.  This is important because some toxicity tests are conducted outside of acceptable parameters but still reveal good data.  An example of this would be if the sample was intentionally conducted at lower than normal temperatures to catch toxicity of pesticides that is higher at lower temperatures and more closely represents the temperature of that water where the sample was collected.
4)	Remove all Samples with StatTest other than t-test – export this data as a table named “Wrong Stat Test” to be assessed by hand if applicable.
5)	For all remaining data convert all SigEffectCodes with the value of SL to be equal to 1.  Convert all other codes to equal zero since SL is the only code that counts as an exceedance.  Cut out all data that does not have a SigEffectCode.  This may, or may not be toxicity data, but it will not be assessed by ReLEP at this time.  The data cut out in this step will be exported as a table called “Missing SigEffectCode” to be reviewed later.
6)	Look up waterbody and beneficial use.  Export all data missing waterbody information as a table named “Missing Stations”
7)	Each Sample Date, Station Code, Matrix combination is a sample.  For each of those combinations calculate the max value.  A zero is a clean sample, a 1 is an exceedance.  Any entry at this point is considered a sample.  This means that if more than one species was tested at this station, on this day, for this matrix, all species together will only be counted as one sample, but if any one of the species exhibits toxicity of any kind that is statistically significant, it will be counted as a toxic sample.  Alternatively, none of the species from a sample can exhibit statistically significant toxic effect in order of the sample to be considered clean.
8)	Create a table with a list of all the species tested and the test conducted on each of the species for each Waterbody, Matrix, ParentProject Name combination.  This information will be inserted into the Data Used section of the LOE with the following generic text; “____ of the ____ samples exhibited ________ toxicity.  A sample may have multiple toxicity test results, but will only be counted once.  A sample is defined as being collected on the same day, at the same location with the same lab sample ID (if provided). The following organisms and parameters were utilized for the toxicity tests: ___________, ___________.”  Where the first two blanks are exceedances and samples respectively, the third blank is the matrix, and fourth and fifth blanks are the species that were tested and the fifth blank is the test that was conducted on the species in the data represented by the LOE.  We could also insert text that specifies which species and test (if any) exhibited toxicity, but I worry that this could get messy if there are many species and tests which exhibited toxicity.
9)	Count up samples and exceedances for each Waterbody, Matrix, ParentProject Name combination.
10)	Create a table that counts up the number of stations within each LOE, and creates a list of station codes contained in the LOE as well.  This information will be inserted into the “Spatial Representation” section of the LOE.
11)	Calculate the range of dates over which the data was collected.  Insert this information into the Temporal Representation field.
12)	Add objective language and objective reference code to each line of data.  This information will be dependent on the Matrix and the Regional Board the data is being assessed for.
13)	Write LOEs (see LOE writing document)

